apiVersion: ray.io/v1
kind: RayService
metadata:
  name: serve
spec:
  serveConfigV2: | # config generated by `serve build`
    applications:
    - name: app
      runtime_env:
        env_vars:
          VLLM_TEST_FORCE_FP8_MARLIN: "1"
      route_prefix: /
      import_path: src.main:deployment
      deployments:
        - name: VLLMDeployment

          autoscaling_config:
            target_ongoing_requests: 5
            min_replicas: 1
            max_replicas: 2
            max_ongoing_requests: 10

          ray_actor_options:
            num_cpus: 0.5
            num_gpus: 0.05

          user_config:
            engine_args:
              model: neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8
              served_model_name: Llama-3
              download_dir: models/
              trust_remote_code: true
              tensor_parallel_size: 1
              worker_use_ray: false
              max_model_len: 16384

  rayClusterConfig:
    rayVersion: "2.34.0" # ray version in the docker image
    enableInTreeAutoscaling: false
    headGroupSpec:
      rayStartParams:
        dashboard-host: "0.0.0.0"
      template:
        spec:
          containers:
            - name: serve-head
              image: ghcr.io/datvodinh/serve:latest
              resources:
                limits:
                  cpu: 2
                  memory: 6Gi
                requests:
                  cpu: 2
                  memory: 6Gi
              ports:
                - containerPort: 6379
                  name: gcs-server
                - containerPort: 8265 # Ray dashboard
                  name: dashboard
                - containerPort: 10001 # Client
                  name: client
                - containerPort: 8000 # FastAPI
                  name: serve

    workerGroupSpecs:
      - replicas: 1
        minReplicas: 1
        maxReplicas: 1
        groupName: small-group
        rayStartParams: {}
        template:
          spec:
            tolerations:
              - key: "gpunode"
                operator: "Equal"
                value: "true"
                effect: "NoExecute"
            containers:
              - name: serve-worker
                image: ghcr.io/datvodinh/serve:latest
                lifecycle:
                  preStop:
                    exec:
                      command: ["/bin/sh", "-c", "ray stop"]
                env:
                  - name: RAY_health_check_initial_delay_ms
                    value: "999999"
                  - name: RAY_health_check_period_ms
                    value: "999999"
                resources:
                  limits:
                    cpu: "8"
                    memory: "24Gi"
                    # nvidia.com/gpu: "1"
                  requests:
                    cpu: "6"
                    memory: "16Gi"
                    # nvidia.com/gpu: "1"
